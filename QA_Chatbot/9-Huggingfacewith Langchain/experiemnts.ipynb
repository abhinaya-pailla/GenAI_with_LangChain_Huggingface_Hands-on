{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face x LangChain : A new partner package in LangChain\n",
    "langchain_huggingface, a partner package in LangChain jointly maintained by Hugging Face and LangChain. This new Python package is designed to bring the power of the latest development of Hugging Face into LangChain and keep it up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_huggingface in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (0.0.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from langchain_huggingface) (0.24.6)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from langchain_huggingface) (0.2.33)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from langchain_huggingface) (3.0.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from langchain_huggingface) (0.19.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from langchain_huggingface) (4.44.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.11.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.1.100)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (8.5.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.4.0+cpu)\n",
      "Requirement already satisfied: numpy in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.14.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (10.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (2024.7.24)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.4)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain_huggingface) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_huggingface) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.7.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain_huggingface) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.0.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (0.24.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from requests->huggingface_hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\paill\\coding\\gen_ai\\end_end_projects\\qa_chatbot\\venv\\lib\\site-packages (from requests->huggingface_hub) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "## API Call\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFaceEndpoint\n",
    "#### How to Access HuggingFace Models with API\n",
    "There are also two ways to use this class. You can specify the model with the repo_id parameter. Those endpoints use the serverless API, which is particularly beneficial to people using pro accounts or enterprise hub. Still, regular users can already have access to a fair amount of request by connecting with their HF token in the environment where they are executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n",
      "c:\\Users\\paill\\Coding\\GEN_AI\\END_END_Projects\\QA_Chatbot\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, model_kwargs={'max_length': 150, 'token': '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'}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?\\n\\nMachine learning is a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It focuses on the development of computer programs that can access data and use it to learn for themselves.\\n\\nThe process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples that we provide. The primary aim is to allow the computers to learn automatically without human intervention or assistance and adjust actions accordingly.\\n\\nMachine learning is a part of the broader category of AI, which includes, for example, natural language processing, expert systems, and robotics.\\n\\nWhy is machine learning important?\\n\\nMachine learning is important because it allows computers to learn from data and make decisions with minimal human intervention. This is particularly useful in situations where the data is too large or complex for humans to process manually.\\n\\nMachine learning has a wide range of applications, including image and speech recognition, recommendation systems, fraud detection, and predictive maintenance. It is also being used in fields such as healthcare, finance, and marketing to help make more accurate predictions and improve decision-making.\\n\\nWhat are the benefits of machine learning?\\n\\nSome of the key benefits of machine learning include:\\n\\n1. Improved decision-making: Machine learning algorithms can analyze large amounts of data and identify patterns and trends that humans might miss. This can help businesses make more informed decisions and improve their bottom line.\\n2. Increased efficiency: Machine learning can automate repetitive tasks, freeing up human resources for more important work. This can lead to increased productivity and cost savings.\\n3. Enhanced customer experience: Machine learning can be used to personalize the customer experience by making recommendations based on individual preferences and behaviors. This can help businesses improve customer satisfaction and loyalty.\\n4. Better predictions: Machine learning algorithms can make more accurate predictions than humans, particularly when it comes to complex data sets. This can help businesses make better decisions and take proactive action.\\n5. Increased accuracy: Machine learning algorithms can be trained on large amounts of data, which can help improve the accuracy of predictions and decisions.\\n\\nWhat are the challenges of machine learning?\\n\\nWhile machine learning has many benefits, there are also some challenges that need to be addressed:\\n\\n1. Data quality: Machine learning algorithms rely on high-quality data to make'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ðŸŽ¨ðŸ§©\\n\\nGenerative AI refers to a class of artificial intelligence models that can create new, original content. These models learn patterns from existing data and use those patterns to generate novel outputs. This can include things like images, music, text, or even 3D models.\\n\\nFor example, a generative AI model could be trained on a large dataset of paintings, and then be asked to create a new painting that is similar in style. Or, it could be trained on a dataset of songs, and then be asked to compose a new piece of music.\\n\\nGenerative AI models are used in a wide range of applications, from creating art and music to generating code and even designing new products. They are also being used to generate realistic images and videos, which can be used for things like creating virtual reality experiences or improving computer graphics in movies and games.\\n\\nOne of the most well-known examples of a generative AI model is DeepMind's AlphaGo, which was able to learn to play the game of Go at a superhuman level by using a combination of deep neural networks and reinforcement learning. Another example is OpenAI's GPT-3, a language model that can generate human-like text based on the input it receives.\\n\\nGenerative AI is a rapidly growing field, and there is much ongoing research and development in this area. It has the potential to revolutionize many industries and create entirely new ones, and it is an exciting area to watch as it continues to evolve.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is generative AI \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='google/gemma-2-9b', temperature=0.7, model_kwargs={'max_length': 150, 'token': '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'}, model='google/gemma-2-9b', client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>, async_client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"google/gemma-2-9b\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": " (Request ID: WEjp-PHWidnraADBdsrpR)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/google/gemma-2-9b.\nIf you are trying to create or update content, make sure you have a token with the `write` role.\nThe model google/gemma-2-9b is too large to be loaded automatically (36GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\paill\\Coding\\GEN_AI\\END_END_Projects\\QA_Chatbot\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\paill\\Coding\\GEN_AI\\END_END_Projects\\QA_Chatbot\\venv\\lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/google/gemma-2-9b",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is machine learning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paill\\Coding\\GEN_AI\\END_END_Projects\\QA_Chatbot\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:385\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    382\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    383\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 385\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    386\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    387\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    388\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    389\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    390\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    391\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    392\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    393\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    394\u001b[0m         )\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    397\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\paill\\Coding\\GEN_AI\\END_END_Projects\\QA_Chatbot\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:750\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    744\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    748\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    749\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\paill\\Coding\\GEN_AI\\END_END_Projects\\QA_Chatbot\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:944\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    930\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    931\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    932\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    942\u001b[0m         )\n\u001b[0;32m    943\u001b[0m     ]\n\u001b[1;32m--> 944\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    945\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\paill\\Coding\\GEN_AI\\END_END_Projects\\QA_Chatbot\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:787\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    786\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    788\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mc:\\Users\\paill\\Coding\\GEN_AI\\END_END_Projects\\QA_Chatbot\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:774\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    766\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    771\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 774\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    775\u001b[0m                 prompts,\n\u001b[0;32m    776\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    777\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    778\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    779\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    780\u001b[0m             )\n\u001b[0;32m    781\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    782\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    783\u001b[0m         )\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\paill\\Coding\\GEN_AI\\END_END_Projects\\QA_Chatbot\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1508\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1505\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1507\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1508\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1509\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1510\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1511\u001b[0m     )\n\u001b[0;32m   1512\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1513\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mc:\\Users\\paill\\Coding\\GEN_AI\\END_END_Projects\\QA_Chatbot\\venv\\lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py:258\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    255\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\paill\\Coding\\GEN_AI\\END_END_Projects\\QA_Chatbot\\venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:304\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\paill\\Coding\\GEN_AI\\END_END_Projects\\QA_Chatbot\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:367\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[0;32m    361\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf you are trying to create or update content, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake sure you have a token with the `write` role.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m:  (Request ID: WEjp-PHWidnraADBdsrpR)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/google/gemma-2-9b.\nIf you are trying to create or update content, make sure you have a token with the `write` role.\nThe model google/gemma-2-9b is too large to be loaded automatically (36GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints)."
     ]
    }
   ],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, model_kwargs={'max_length': 150, 'token': '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'}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] template='\\nQuestion:{question}\\nAnswer:Lets think step by step.\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate,LLMChain\n",
    "template=\"\"\"\n",
    "Question:{question}\n",
    "Answer:Lets think step by step.\n",
    "\"\"\"\n",
    "prompt=PromptTemplate(template=template,input_variables=[\"question\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"?\\n\\nIndia\\nIndia won the Cricket World Cup for the second time by beating Sri Lanka by six wickets in the final at Wankhede Stadium, Mumbai, on 2 April 2011.\\n\\nWho won the first cricket World Cup?\\n\\nIndia\\nThe first Cricket World Cup was held in England in 1975. The tournament was won by West Indies, who defeated Australia in the final at Lord's Cricket Ground by 17 runs. India finished in fifth place in the tournament.\\n\\nWho won the 2019 Cricket World Cup?\\n\\nEngland\\nEngland won the Cricket World Cup for the first time by defeating New Zealand by a single run in a dramatic final at Lord's Cricket Ground on 14 July 2019. The match was tied after 100 overs, with England scoring 241 and New Zealand replying with 241 for 8.\\n\\nWho won the 2015 Cricket World Cup?\\n\\nAustralia\\nAustralia won the Cricket World Cup for the fifth time by defeating New Zealand in the final at the Melbourne Cricket Ground on 29 March 2015. The match was a one-day international (ODI) and was the culmination of the 2015 Cricket World Cup, which was held in Australia and New Zealand.\\n\\nWho won the 2017 ICC Champions Trophy?\\n\\nPakistan\\nPakistan won the ICC Champions Trophy for the first time by defeating India in the final at The Oval on 18 June 2017. The match was a one-day international (ODI) and was the culmination of the 2017 ICC Champions Trophy, which was held in England.\\n\\nWho won the 2019 T20 World Cup?\\n\\nPakistan\\nPakistan won the T20 World Cup for the first time by defeating England in the final at the Melbourne Cricket Ground on 15 November 2021. The match was a T20 international (T20I) and was the culmination of the 2021 T20 World Cup, which was held in the United Arab Emirates and Oman\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain=LLMChain(llm=llm,prompt=prompt)\n",
    "llm.invoke(\"Who won the cricket World up 2011\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paill\\Coding\\GEN_AI\\END_END_Projects\\QA_Chatbot\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\paill\\.cache\\huggingface\\hub\\models--BAAI--bge-small-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = hf.embed_query(\"hi this is harrison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.028416551649570465,\n",
       " 0.01218324899673462,\n",
       " 0.027443984523415565,\n",
       " -0.05482873693108559,\n",
       " 0.024238910526037216,\n",
       " 0.0007662189891561866,\n",
       " 0.06783369183540344,\n",
       " 0.016348352655768394,\n",
       " -0.018950756639242172,\n",
       " 0.012542912736535072,\n",
       " 0.021565034985542297,\n",
       " -0.08793043345212936,\n",
       " 0.0006460760487243533,\n",
       " 0.03327080234885216,\n",
       " 0.005463745910674334,\n",
       " -0.060376446694135666,\n",
       " 0.050422631204128265,\n",
       " 0.004434794653207064,\n",
       " 0.0009598954347893596,\n",
       " 0.0017405633116140962,\n",
       " 0.0032988376915454865,\n",
       " 0.03167250379920006,\n",
       " -0.04880750551819801,\n",
       " -0.044819146394729614,\n",
       " 0.07132107764482498,\n",
       " -0.007510819938033819,\n",
       " -0.001125917537137866,\n",
       " -0.01580118015408516,\n",
       " -0.029402390122413635,\n",
       " -0.17224566638469696,\n",
       " -0.03189518675208092,\n",
       " -0.001629116479307413,\n",
       " 0.018105048686265945,\n",
       " 0.015315357595682144,\n",
       " -0.02072959393262863,\n",
       " -0.008873010985553265,\n",
       " -0.0012822377029806376,\n",
       " 0.027276957407593727,\n",
       " -0.010114278644323349,\n",
       " 0.012621660716831684,\n",
       " -0.0070778424851596355,\n",
       " -0.016693223267793655,\n",
       " 0.04085586592555046,\n",
       " 0.0239383727312088,\n",
       " -0.020081544294953346,\n",
       " 0.028681132942438126,\n",
       " -0.01940074935555458,\n",
       " -0.01461817231029272,\n",
       " 0.017379652708768845,\n",
       " 0.0041640945710241795,\n",
       " 0.06415645778179169,\n",
       " 0.04768303781747818,\n",
       " 0.0018365196883678436,\n",
       " -8.064539724728093e-05,\n",
       " 0.016596831381320953,\n",
       " 0.011124239303171635,\n",
       " 0.0696944072842598,\n",
       " 0.05182056128978729,\n",
       " 0.055685319006443024,\n",
       " 0.05551544204354286,\n",
       " 0.000503949704580009,\n",
       " 0.0418706089258194,\n",
       " -0.1534409075975418,\n",
       " 0.05180787295103073,\n",
       " 0.006689780857414007,\n",
       " -0.0316707007586956,\n",
       " -0.009104978293180466,\n",
       " -0.05160471424460411,\n",
       " 0.04250861331820488,\n",
       " 0.028200017288327217,\n",
       " -0.010748111642897129,\n",
       " 0.02240583300590515,\n",
       " 0.044395461678504944,\n",
       " 0.004115520976483822,\n",
       " 0.018998438492417336,\n",
       " -0.0043571931309998035,\n",
       " 0.04762762784957886,\n",
       " 0.011824669316411018,\n",
       " 0.008164634928107262,\n",
       " 0.008177232928574085,\n",
       " -0.009698718786239624,\n",
       " -0.014260252937674522,\n",
       " 0.011409704573452473,\n",
       " -0.07362116873264313,\n",
       " -0.05439520999789238,\n",
       " -0.05703959986567497,\n",
       " -0.003608542028814554,\n",
       " 0.00266610411927104,\n",
       " 0.02378247305750847,\n",
       " 0.01537624467164278,\n",
       " -0.07020372152328491,\n",
       " -0.03130031377077103,\n",
       " -0.0031143161468207836,\n",
       " -0.01581212505698204,\n",
       " -0.03791403770446777,\n",
       " -0.025921950116753578,\n",
       " 0.018168432638049126,\n",
       " -0.038824524730443954,\n",
       " -0.05674510821700096,\n",
       " 0.5792059898376465,\n",
       " -0.05278836190700531,\n",
       " 0.020716356113553047,\n",
       " 0.06794393062591553,\n",
       " -0.04541649669408798,\n",
       " 0.011642489582300186,\n",
       " -0.021571751683950424,\n",
       " 0.02034170739352703,\n",
       " -0.02744891494512558,\n",
       " -0.04558899253606796,\n",
       " -0.029443584382534027,\n",
       " -0.02366250939667225,\n",
       " -0.0343153178691864,\n",
       " 0.001938825473189354,\n",
       " -0.07095137983560562,\n",
       " 0.03455634415149689,\n",
       " -0.030558917671442032,\n",
       " 0.039078567177057266,\n",
       " -0.02970734052360058,\n",
       " -0.0008282799390144646,\n",
       " -0.01215940248221159,\n",
       " -0.018272846937179565,\n",
       " 0.025486532598733902,\n",
       " -0.00446169450879097,\n",
       " 0.01633531227707863,\n",
       " 0.019126448780298233,\n",
       " -0.05483211204409599,\n",
       " 0.02763596922159195,\n",
       " -0.0047576334327459335,\n",
       " 0.05900166556239128,\n",
       " -0.0016944401431828737,\n",
       " 0.008014999330043793,\n",
       " -0.03772678226232529,\n",
       " -0.0989304631948471,\n",
       " -0.022574400529265404,\n",
       " -0.03760465979576111,\n",
       " -0.002169891959056258,\n",
       " 0.003244596067816019,\n",
       " -0.019202515482902527,\n",
       " -0.008631201460957527,\n",
       " -0.0480230413377285,\n",
       " 0.00869670044630766,\n",
       " -0.09516111016273499,\n",
       " -0.03496044874191284,\n",
       " -0.043607961386442184,\n",
       " -0.00034400858567096293,\n",
       " -0.010173656046390533,\n",
       " -0.030999552458524704,\n",
       " 0.024309683591127396,\n",
       " -0.020402029156684875,\n",
       " 0.03113941103219986,\n",
       " 0.0008811246370896697,\n",
       " 0.01391653623431921,\n",
       " -0.031196244060993195,\n",
       " -0.03715403378009796,\n",
       " 0.0040296404622495174,\n",
       " 0.014799805358052254,\n",
       " 0.04318896308541298,\n",
       " 0.03875484690070152,\n",
       " 0.013851999305188656,\n",
       " 0.019797835499048233,\n",
       " 0.010267077013850212,\n",
       " -0.005434125661849976,\n",
       " -0.014299207367002964,\n",
       " 0.027637839317321777,\n",
       " 0.009802618995308876,\n",
       " -0.1355029046535492,\n",
       " -0.017139749601483345,\n",
       " 0.0176171213388443,\n",
       " 0.023132212460041046,\n",
       " 0.0017590154893696308,\n",
       " 0.030889473855495453,\n",
       " 0.039918698370456696,\n",
       " -0.013684204779565334,\n",
       " 0.02481650747358799,\n",
       " 0.05405023694038391,\n",
       " 0.017761144787073135,\n",
       " -0.018475068733096123,\n",
       " 0.025955356657505035,\n",
       " -0.006377530284225941,\n",
       " -0.016587313264608383,\n",
       " 0.03784800320863724,\n",
       " -0.027290020138025284,\n",
       " -0.0528457835316658,\n",
       " -0.03803319111466408,\n",
       " 0.05191107839345932,\n",
       " -0.007557082921266556,\n",
       " -0.03180534765124321,\n",
       " 0.013284197077155113,\n",
       " -0.027723757550120354,\n",
       " 0.056306514889001846,\n",
       " 0.003041847376152873,\n",
       " 0.05332478880882263,\n",
       " -0.057911280542612076,\n",
       " -0.011325870640575886,\n",
       " -0.031172044575214386,\n",
       " 0.02560870535671711,\n",
       " 0.03389059752225876,\n",
       " -0.0010284583549946547,\n",
       " 0.015864910557866096,\n",
       " 0.01059520523995161,\n",
       " -0.027037756517529488,\n",
       " -0.0009308441076427698,\n",
       " -0.04815226420760155,\n",
       " 0.028179265558719635,\n",
       " 0.010320658795535564,\n",
       " 0.06662959605455399,\n",
       " -0.016558179631829262,\n",
       " -0.004431372042745352,\n",
       " 0.03823429346084595,\n",
       " -0.02340816520154476,\n",
       " -0.035581715404987335,\n",
       " -0.05829070135951042,\n",
       " -0.0111814821138978,\n",
       " -0.017684604972600937,\n",
       " -0.01614130102097988,\n",
       " -0.0342453308403492,\n",
       " -0.025139562785625458,\n",
       " 0.03939669951796532,\n",
       " -0.02365822158753872,\n",
       " -0.007725025061517954,\n",
       " -0.005098921712487936,\n",
       " -0.03523427993059158,\n",
       " -0.014076865278184414,\n",
       " -0.2232602834701538,\n",
       " -0.03147130459547043,\n",
       " -0.0012906169285997748,\n",
       " -0.0017200090223923326,\n",
       " -0.00784607045352459,\n",
       " -0.05802328139543533,\n",
       " 0.046174533665180206,\n",
       " 0.02455267496407032,\n",
       " 0.07320841401815414,\n",
       " 0.017268333584070206,\n",
       " 0.047612085938453674,\n",
       " 0.013473335653543472,\n",
       " -0.00551605224609375,\n",
       " -0.01435784064233303,\n",
       " -0.009674325585365295,\n",
       " 0.04878251254558563,\n",
       " 0.030538110062479973,\n",
       " -0.024993931874632835,\n",
       " 0.02148623950779438,\n",
       " 0.01763983629643917,\n",
       " 0.05313889682292938,\n",
       " 0.013485018163919449,\n",
       " -0.0232259351760149,\n",
       " -0.021403998136520386,\n",
       " 0.02607536129653454,\n",
       " 0.0020291798282414675,\n",
       " 0.12753744423389435,\n",
       " 0.08316841721534729,\n",
       " 0.044089484959840775,\n",
       " -0.02670358680188656,\n",
       " 0.005521978251636028,\n",
       " -0.009294905699789524,\n",
       " 0.020074300467967987,\n",
       " -0.09684181958436966,\n",
       " -0.024703893810510635,\n",
       " 0.02508702129125595,\n",
       " 0.0020886296406388283,\n",
       " -0.044894058257341385,\n",
       " -0.078611359000206,\n",
       " -0.004376337397843599,\n",
       " -0.06590455770492554,\n",
       " 0.014689371921122074,\n",
       " -0.057641856372356415,\n",
       " -0.07152023166418076,\n",
       " -0.062326543033123016,\n",
       " 0.0034316384699195623,\n",
       " -0.046065520495176315,\n",
       " 0.045300837606191635,\n",
       " -0.02676231786608696,\n",
       " 0.03401095047593117,\n",
       " 0.04547380283474922,\n",
       " -0.028179241344332695,\n",
       " 0.005011778324842453,\n",
       " 0.009630822576582432,\n",
       " -0.030305594205856323,\n",
       " -0.03612491488456726,\n",
       " -0.013626985251903534,\n",
       " -0.03265373408794403,\n",
       " -0.044677507132291794,\n",
       " 0.010642224922776222,\n",
       " -0.027486342936754227,\n",
       " -0.024565119296312332,\n",
       " -0.024747803807258606,\n",
       " 0.053619518876075745,\n",
       " 0.02078990265727043,\n",
       " 0.0194685235619545,\n",
       " 0.05324120447039604,\n",
       " -0.014002423733472824,\n",
       " 0.02124328911304474,\n",
       " -0.04957331717014313,\n",
       " -0.008522610180079937,\n",
       " 0.007852812297642231,\n",
       " -0.05719398707151413,\n",
       " -0.027550654485821724,\n",
       " 0.005300870165228844,\n",
       " 0.040072906762361526,\n",
       " 0.019597889855504036,\n",
       " -0.04519732668995857,\n",
       " 0.0324358232319355,\n",
       " -0.012342470698058605,\n",
       " 0.03431437537074089,\n",
       " 0.02110208198428154,\n",
       " 0.03984649106860161,\n",
       " 0.03166380152106285,\n",
       " -0.033590223640203476,\n",
       " 0.03164792060852051,\n",
       " -0.00330451806075871,\n",
       " 0.004641877021640539,\n",
       " 0.037589412182569504,\n",
       " -0.05924459546804428,\n",
       " 0.0070283301174640656,\n",
       " 0.0038087344728410244,\n",
       " -0.02578885480761528,\n",
       " -0.021203376352787018,\n",
       " 0.022691218182444572,\n",
       " -0.021772950887680054,\n",
       " -0.27963778376579285,\n",
       " 0.007267373148351908,\n",
       " 0.0210720282047987,\n",
       " 0.04519747570157051,\n",
       " -0.020534489303827286,\n",
       " 0.024313664063811302,\n",
       " -0.0006136825541034341,\n",
       " -0.011857080273330212,\n",
       " -0.03296777606010437,\n",
       " 0.0358431413769722,\n",
       " 0.03128170222043991,\n",
       " 0.06373961269855499,\n",
       " 0.04654787480831146,\n",
       " -0.014470509253442287,\n",
       " 0.015869762748479843,\n",
       " 0.033971309661865234,\n",
       " 0.018059618771076202,\n",
       " 0.002298719948157668,\n",
       " 0.01654987782239914,\n",
       " -0.021714884787797928,\n",
       " -0.034859973937273026,\n",
       " -0.0008649034425616264,\n",
       " 0.15126046538352966,\n",
       " -0.024536726996302605,\n",
       " 0.030671266838908195,\n",
       " -0.007318197749555111,\n",
       " -0.006135482806712389,\n",
       " 0.06415148824453354,\n",
       " 0.0160214826464653,\n",
       " -0.03636447712779045,\n",
       " 0.019898604601621628,\n",
       " -0.02117236517369747,\n",
       " 0.048294179141521454,\n",
       " -0.04478100314736366,\n",
       " 0.0476338192820549,\n",
       " 0.0007749245269224048,\n",
       " -0.005927987862378359,\n",
       " 0.06154266372323036,\n",
       " 0.023968437686562538,\n",
       " 0.013304932974278927,\n",
       " 0.02268453687429428,\n",
       " 0.014538048766553402,\n",
       " -0.05215906724333763,\n",
       " -0.03274961933493614,\n",
       " 0.08583348989486694,\n",
       " -0.003724814625456929,\n",
       " 0.0013494070153683424,\n",
       " 0.04091987758874893,\n",
       " 0.011659692972898483,\n",
       " 0.058436207473278046,\n",
       " -0.022286122664809227,\n",
       " -0.011520694009959698,\n",
       " 0.0047057149931788445,\n",
       " 0.04718263819813728,\n",
       " -0.0019179099472239614,\n",
       " 0.0330093652009964,\n",
       " -0.03505059331655502,\n",
       " -0.020736565813422203,\n",
       " -0.009222164750099182,\n",
       " 0.014618217945098877,\n",
       " 0.006456020753830671,\n",
       " 0.0010978035861626267,\n",
       " 0.01022399589419365,\n",
       " 0.08537214249372482,\n",
       " 0.038839489221572876]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
